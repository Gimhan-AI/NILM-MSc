{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REFIT Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "DATASET_NAME = \"refit\"\n",
    "DATA_DIRECTORY = 'data/refit/'\n",
    "AGG_MEAN = 522\n",
    "AGG_STD = 814\n",
    "APPLIANCE_NAME = 'washingmachine'\n",
    "SAVE_PATH = DATASET_NAME + '/' + APPLIANCE_NAME + '/'\n",
    "\n",
    "params_appliance = {\n",
    "    'kettle': {\n",
    "        'windowlength': 599,\n",
    "        'on_power_threshold': 2000,\n",
    "        'max_on_power': 3998,\n",
    "        'mean': 700,\n",
    "        'std': 1000,\n",
    "        's2s_length': 128,\n",
    "        'houses': [2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 19, 20],\n",
    "        'channels': [8, 9, 9, 8, 7, 9, 9, 7, 6, 9, 5, 9],\n",
    "        'test_house': 2,\n",
    "        'validation_house': 5,\n",
    "        'test_on_train_house': 5,\n",
    "    },\n",
    "    'microwave': {\n",
    "        'windowlength': 599,\n",
    "        'on_power_threshold': 200,\n",
    "        'max_on_power': 3969,\n",
    "        'mean': 500,\n",
    "        'std': 800,\n",
    "        's2s_length': 128,\n",
    "        'houses': [4, 10, 12, 17, 19],\n",
    "        'channels': [8, 8, 3, 7, 4],\n",
    "        'test_house': 4,\n",
    "        'validation_house': 17,\n",
    "        'test_on_train_house': 10,\n",
    "    },\n",
    "    'fridge': {\n",
    "        'windowlength': 599,\n",
    "        'on_power_threshold': 50,\n",
    "        'max_on_power': 3323,\n",
    "        'mean': 200,\n",
    "        'std': 400,\n",
    "        's2s_length': 512,\n",
    "        'houses': [2, 5, 9, 12, 15],\n",
    "        'channels': [1, 1, 1,  1, 1],\n",
    "        'test_house': 15,\n",
    "        'validation_house': 12,\n",
    "        'test_on_train_house': 5,\n",
    "    },\n",
    "    'dishwasher': {\n",
    "        'windowlength': 599,\n",
    "        'on_power_threshold': 10,\n",
    "        'max_on_power': 3964,\n",
    "        'mean': 700,\n",
    "        'std': 1000,\n",
    "        's2s_length': 1536,\n",
    "        'houses': [5, 7, 9, 13, 16, 18, 20],\n",
    "        'channels': [4, 6, 4, 4, 6, 6, 5],\n",
    "        'test_house': 20,\n",
    "        'validation_house': 18,\n",
    "        'test_on_train_house': 13,\n",
    "    },\n",
    "    'washingmachine': {\n",
    "        'windowlength': 599,\n",
    "        'on_power_threshold': 20,\n",
    "        'max_on_power': 3999,\n",
    "        'mean': 400,\n",
    "        'std': 700,\n",
    "        's2s_length': 2000,\n",
    "        'houses': [2, 5, 7, 8, 9, 15, 16, 17, 18],\n",
    "        'channels': [2, 3, 5, 4, 3, 3, 5, 4, 5],\n",
    "        'test_house': 8,\n",
    "        'validation_house': 18,\n",
    "        'test_on_train_house': 5,\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def load(path, building, appliance, channel):\n",
    "\n",
    "    # load csv\n",
    "    file_name = path + 'CLEAN_House' + str(building) + '.csv'\n",
    "    single_csv = pd.read_csv(file_name,\n",
    "                         header=0,\n",
    "                         names=['aggregate', appliance],\n",
    "                         usecols=[2, channel+2],\n",
    "                         na_filter=False,\n",
    "                         parse_dates=True,\n",
    "                         memory_map=True)\n",
    "\n",
    "    return single_csv\n",
    "\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()        \n",
    "    \n",
    "    appliance_name = APPLIANCE_NAME\n",
    "    print(appliance_name)\n",
    "    \n",
    "    path = DATA_DIRECTORY\n",
    "    save_path = SAVE_PATH\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    print(path)\n",
    "    aggregate_mean = AGG_MEAN\n",
    "    aggregate_std = AGG_STD  \n",
    "    \n",
    "    total_length = 0\n",
    "    print(\"Starting creating dataset...\")\n",
    "    # Looking for proper files\n",
    "    for idx, filename in enumerate(os.listdir(path)):\n",
    "        single_step_time = time.time()\n",
    "    \n",
    "        if filename == 'CLEAN_House' + str(params_appliance[appliance_name]['test_house']) + '.csv':\n",
    "            print('File: ' + filename + ' test set')\n",
    "            # Loading\n",
    "            test = load(path,\n",
    "                 params_appliance[appliance_name]['test_house'],\n",
    "                 appliance_name,\n",
    "                 params_appliance[appliance_name]['channels'][params_appliance[appliance_name]['houses']\n",
    "                        .index(params_appliance[appliance_name]['test_house'])]\n",
    "                 )\n",
    "    \n",
    "            # Normalization\n",
    "            test['aggregate'] = (test['aggregate'] - aggregate_mean) / aggregate_std\n",
    "            test[appliance_name] = \\\n",
    "                (test[appliance_name] - params_appliance[appliance_name]['mean']) / params_appliance[appliance_name]['std']\n",
    "    \n",
    "            # Save\n",
    "            test.to_csv(save_path + appliance_name + '_test_' + 'H' + str(params_appliance[appliance_name]['test_house'])\n",
    "                        + '.csv', index=False)\n",
    "    \n",
    "            print(\"Size of test set is {:.3f} M rows (House {:d}).\"\n",
    "                  .format(test.shape[0] / 10 ** 6, params_appliance[appliance_name]['test_house']))\n",
    "            del test\n",
    "    \n",
    "        elif filename == 'CLEAN_House' + str(params_appliance[appliance_name]['validation_house']) + '.csv':\n",
    "            print('File: ' + filename + ' validation set')\n",
    "            # Loading\n",
    "            val = load(path,\n",
    "                 params_appliance[appliance_name]['validation_house'],\n",
    "                 appliance_name,\n",
    "                 params_appliance[appliance_name]['channels']\n",
    "                 [params_appliance[appliance_name]['houses']\n",
    "                        .index(params_appliance[appliance_name]['validation_house'])]\n",
    "                 )\n",
    "    \n",
    "            # Normalization\n",
    "            val['aggregate'] = (val['aggregate'] - aggregate_mean) / aggregate_std\n",
    "            val[appliance_name] = \\\n",
    "                (val[appliance_name] - params_appliance[appliance_name]['mean']) / params_appliance[appliance_name]['std']\n",
    "    \n",
    "            # Save\n",
    "            val.to_csv(save_path + appliance_name + '_validation_' + 'H' + str(params_appliance[appliance_name]['validation_house'])\n",
    "                       + '.csv', index=False)\n",
    "    \n",
    "            print(\"Size of validation set is {:.3f} M rows (House {:d}).\"\n",
    "                  .format(val.shape[0] / 10 ** 6, params_appliance[appliance_name]['validation_house']))\n",
    "            del val\n",
    "    \n",
    "        elif int(re.search(r'\\d+', filename).group()) in params_appliance[appliance_name]['houses']:\n",
    "            print('File: ' + filename)\n",
    "            print('    House: ' + re.search(r'\\d+', filename).group())\n",
    "    \n",
    "            # Loading\n",
    "            try:\n",
    "                csv = load(path,\n",
    "                           int(re.search(r'\\d+', filename).group()),\n",
    "                           appliance_name,\n",
    "                           params_appliance[appliance_name]['channels']\n",
    "                           [params_appliance[appliance_name]['houses']\n",
    "                                  .index(int(re.search(r'\\d+', filename).group()))]\n",
    "                           )\n",
    "    \n",
    "                # Normalization\n",
    "                csv['aggregate'] = (csv['aggregate'] - aggregate_mean) / aggregate_std\n",
    "                csv[appliance_name] = \\\n",
    "                    (csv[appliance_name] - params_appliance[appliance_name]['mean']) / params_appliance[appliance_name][\n",
    "                        'std']\n",
    "    \n",
    "                rows, columns = csv.shape\n",
    "                total_length += rows\n",
    "    \n",
    "                if filename == 'CLEAN_House' + str(params_appliance[appliance_name]['test_on_train_house']) + '.csv':\n",
    "                    csv.to_csv(save_path + appliance_name + '_test_on_train_' + 'H' + str(\n",
    "                        params_appliance[appliance_name]['test_on_train_house'])\n",
    "                               + '.csv', index=False)\n",
    "                    print(\"Size of test on train set is {:.3f} M rows (House {:d}).\"\n",
    "                          .format(csv.shape[0] / 10 ** 6, params_appliance[appliance_name]['test_on_train_house']))\n",
    "    \n",
    "                # saving the whole merged file\n",
    "                csv.to_csv(save_path + appliance_name + '_training_.csv', mode='a', index=False, header=False)\n",
    "    \n",
    "                del csv\n",
    "    \n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    print(\"Size of training set is {:.3f} M rows.\".format(total_length / 10 ** 6))\n",
    "    print(\"\\nNormalization parameters: \")\n",
    "    print(\"Mean and standard deviation values USED for AGGREGATE are:\")\n",
    "    print(\"    Mean = {:d}, STD = {:d}\".format(aggregate_mean, aggregate_std))\n",
    "    print('Mean and standard deviation values USED for ' + appliance_name + ' are:')\n",
    "    print(\"    Mean = {:d}, STD = {:d}\"\n",
    "          .format(params_appliance[appliance_name]['mean'], params_appliance[appliance_name]['std']))\n",
    "    print(\"\\nTraining, validation and test sets are  in: \" + save_path)\n",
    "    print(\"Total elapsed time: {:.2f} min.\".format((time.time() - start_time) / 60))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NILM Sequence to Point Model Training with GPU on REFIT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "import glob\n",
    "\n",
    "# Parameters\n",
    "appliance_name = \"washingmachine\" # kettle, dishwasher, fridge, microwave, washingmachine\n",
    "dataset_name = \"refit\"\n",
    "batch_size = 32\n",
    "crop = 1000\n",
    "network_type = \"seq2point\"\n",
    "training_directory = dataset_name + \"/\" + appliance_name + \"/\" + appliance_name + \"_training_.csv\"\n",
    "validation_directory = glob.glob(f\"{dataset_name}/{appliance_name}/{appliance_name}_validation_H*.csv\")[0]\n",
    "save_model_dir = \"saved_models/\" + appliance_name + \"_\" + network_type + \"_model.keras\"\n",
    "epochs = 10\n",
    "input_window_length = 599\n",
    "validation_frequency = 1\n",
    "patience = 3\n",
    "min_delta = 1e-6\n",
    "verbose = 1\n",
    "loss = \"mse\"\n",
    "metrics = [\"mse\", \"msle\", \"mae\"]\n",
    "learning_rate = 0.001\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "shuffle = True\n",
    "\n",
    "window_size = 2 + input_window_length\n",
    "window_offset = int((0.5 * window_size) - 1)\n",
    "offset = window_offset\n",
    "max_chunk_size = 5 * 10 ** 2\n",
    "ram_threshold = 5 * 10 ** 5\n",
    "skip_rows_train = 10000000\n",
    "validation_steps = 100\n",
    "skip_rows_val = 0\n",
    "\n",
    "# Data Loader Function\n",
    "class TrainSlidingWindowGenerator:\n",
    "    def __init__(self, file_name, chunk_size, shuffle, offset, batch_size=1000, crop=100000, skip_rows=0, ram_threshold=5 * 10 ** 5):\n",
    "        self.file_name = file_name\n",
    "        self.batch_size = batch_size\n",
    "        self.chunk_size = chunk_size\n",
    "        self.shuffle = shuffle\n",
    "        self.offset = offset\n",
    "        self.crop = crop\n",
    "        self.skip_rows = skip_rows\n",
    "        self.ram_threshold = ram_threshold\n",
    "        self.total_size = 0\n",
    "        self.total_num_samples = crop\n",
    "\n",
    "    def load_dataset(self):\n",
    "        print(\"Loading dataset from: \", self.file_name)\n",
    "        data_array = np.array(pd.read_csv(self.file_name, nrows=self.crop, skiprows=self.skip_rows, header=0))\n",
    "        inputs = data_array[:, 0]\n",
    "        outputs = data_array[:, 1]\n",
    "        maximum_batch_size = inputs.size - 2 * self.offset\n",
    "        self.total_num_samples = maximum_batch_size\n",
    "        if self.batch_size < 0:\n",
    "            self.batch_size = maximum_batch_size\n",
    "\n",
    "        indicies = np.arange(maximum_batch_size)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indicies)\n",
    "\n",
    "        while True:\n",
    "            for start_index in range(0, maximum_batch_size, self.batch_size):\n",
    "                splice = indicies[start_index : start_index + self.batch_size]\n",
    "                input_data = np.array([inputs[index : index + 2 * self.offset + 1] for index in splice])\n",
    "                output_data = outputs[splice + self.offset].reshape(-1, 1)\n",
    "                yield input_data, output_data\n",
    "\n",
    "# Model Creation Function\n",
    "def create_model(input_window_length):\n",
    "    input_layer = tf.keras.layers.Input(shape=(input_window_length,))\n",
    "    reshape_layer = tf.keras.layers.Reshape((1, input_window_length, 1))(input_layer)\n",
    "    conv_layer_1 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(10, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(reshape_layer)\n",
    "    conv_layer_2 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(8, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_1)\n",
    "    conv_layer_3 = tf.keras.layers.Convolution2D(filters=40, kernel_size=(6, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_2)\n",
    "    conv_layer_4 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_3)\n",
    "    conv_layer_5 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_4)\n",
    "    flatten_layer = tf.keras.layers.Flatten()(conv_layer_5)\n",
    "    label_layer = tf.keras.layers.Dense(1024, activation=\"relu\")(flatten_layer)\n",
    "    output_layer = tf.keras.layers.Dense(1, activation=\"linear\")(label_layer)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "# Load Dataset for Training\n",
    "training_chunker = TrainSlidingWindowGenerator(file_name=training_directory, \n",
    "                                               chunk_size=max_chunk_size,\n",
    "                                               batch_size=batch_size,\n",
    "                                               crop=crop,\n",
    "                                               shuffle=shuffle,\n",
    "                                               skip_rows=skip_rows_train,\n",
    "                                               offset=offset,\n",
    "                                               ram_threshold=ram_threshold)\n",
    "\n",
    "# Load Dataset for Validation\n",
    "validation_chunker = TrainSlidingWindowGenerator(file_name=validation_directory, \n",
    "                                                 chunk_size=max_chunk_size,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 crop=crop,\n",
    "                                                 shuffle=False,\n",
    "                                                 skip_rows=skip_rows_val,\n",
    "                                                 offset=offset,\n",
    "                                                 ram_threshold=ram_threshold)\n",
    "\n",
    "# Create and Compile the Model\n",
    "model = create_model(input_window_length)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2), \n",
    "              loss=loss, \n",
    "              metrics=metrics)\n",
    "\n",
    "# Set Early Stopping Callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", min_delta=min_delta, patience=patience, verbose=verbose, mode=\"auto\")\n",
    "callbacks = [early_stopping]\n",
    "\n",
    "# Calculate Steps per Epoch\n",
    "steps_per_training_epoch = np.round(int(training_chunker.total_num_samples / batch_size), decimals=0) if training_chunker.total_num_samples is not None else 1\n",
    "\n",
    "# Train the Model\n",
    "training_history = model.fit(training_chunker.load_dataset(),\n",
    "                             validation_data=validation_chunker.load_dataset(),\n",
    "                             steps_per_epoch=steps_per_training_epoch,\n",
    "                             epochs=epochs,\n",
    "                             verbose=verbose,\n",
    "                             callbacks=callbacks,\n",
    "                             validation_steps=validation_steps,\n",
    "                             validation_freq=validation_frequency)\n",
    "\n",
    "# Save the Model\n",
    "if not os.path.exists(save_model_dir):\n",
    "    open((save_model_dir), 'a').close()\n",
    "model.save(save_model_dir)\n",
    "\n",
    "# Plot Training Results\n",
    "plt.plot(training_history.history[\"loss\"], label=\"MSE (Training Loss)\")\n",
    "if \"val_loss\" in training_history.history:\n",
    "    plt.plot(training_history.history[\"val_loss\"], label=\"MSE (Validation Loss)\")\n",
    "plt.title('Training History')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "del model\n",
    "# del test_input, test_target, input_data, testing_history\n",
    "gc.collect()\n",
    "K.clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NILM Sequence to Point Model Inference with GPU on REFIT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import glob\n",
    "\n",
    "# Parameters\n",
    "appliance_name = \"washingmachine\" # kettle, dishwasher, fridge, microwave, washingmachine\n",
    "dataset_name = \"refit\"\n",
    "batch_size = 1000\n",
    "crop = 10000\n",
    "network_type = \"seq2point\"\n",
    "input_window_length = 599\n",
    "test_directory = glob.glob(f\"{dataset_name}/{appliance_name}/{appliance_name}_test_H*.csv\")[0]\n",
    "saved_model_dir = \"saved_models/\" + appliance_name + \"_\" + network_type + \"_model.keras\"\n",
    "log_file_dir = \"saved_models/\" + appliance_name + \"_\" + network_type + \".log\"\n",
    "\n",
    "# Appliance data\n",
    "appliance_data = {\n",
    "    \"kettle\": {\"mean\": 700, \"std\": 1000},\n",
    "    \"fridge\": {\"mean\": 200, \"std\": 400},\n",
    "    \"dishwasher\": {\"mean\": 700, \"std\": 1000},\n",
    "    \"washingmachine\": {\"mean\": 400, \"std\": 700},\n",
    "    \"microwave\": {\"mean\": 500, \"std\": 800},\n",
    "}\n",
    "\n",
    "# Mains data\n",
    "mains_data = {\"mean\": 522, \"std\": 814}\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(filename=log_file_dir, level=logging.INFO)\n",
    "\n",
    "# Load the test dataset\n",
    "data_frame = pd.read_csv(test_directory, nrows=crop, skiprows=0, header=0)\n",
    "test_input = np.round(np.array(data_frame.iloc[:, 0], float), 6)\n",
    "test_target = np.round(np.array(data_frame.iloc[:, 1], float), 6)\n",
    "del data_frame\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model(saved_model_dir)\n",
    "\n",
    "# Create sliding windows for inference\n",
    "window_size = input_window_length + 2\n",
    "window_offset = int(0.5 * window_size - 1)\n",
    "number_of_windows = 100\n",
    "maximum_batch_size = test_input.size - 2 * window_offset\n",
    "indicies = np.arange(maximum_batch_size)\n",
    "input_data = np.array([test_input[index : index + 2 * window_offset + 1] for index in indicies[:number_of_windows]])\n",
    "\n",
    "# Perform inference\n",
    "start_time = time.time()\n",
    "testing_history = model.predict(input_data, batch_size=batch_size, verbose=2)\n",
    "end_time = time.time()\n",
    "test_time = end_time - start_time\n",
    "\n",
    "# Log inference time\n",
    "inference_log = \"Inference Time: \" + str(test_time)\n",
    "logging.info(inference_log)\n",
    "\n",
    "# Denormalize the results\n",
    "testing_history = ((testing_history * appliance_data[appliance_name][\"std\"]) + appliance_data[appliance_name][\"mean\"])\n",
    "test_target = ((test_target * appliance_data[appliance_name][\"std\"]) + appliance_data[appliance_name][\"mean\"])\n",
    "test_agg = (test_input.flatten() * mains_data[\"std\"]) + mains_data[\"mean\"]\n",
    "test_agg = test_agg[:maximum_batch_size]\n",
    "\n",
    "# Remove negative values\n",
    "test_target[test_target < 0] = 0\n",
    "testing_history[testing_history < 0] = 0\n",
    "test_input[test_input < 0] = 0\n",
    "\n",
    "# Plot results\n",
    "plt.figure(1)\n",
    "plt.plot(test_agg[window_offset: -window_offset], label=\"Aggregate\")\n",
    "plt.plot(test_target[:test_agg.size - (2 * window_offset)], label=\"Ground Truth\")\n",
    "plt.plot(testing_history[:maximum_batch_size], label=\"Predicted\")\n",
    "plt.title(appliance_name + \" \" + network_type)\n",
    "plt.ylabel(\"Power Value (Watts)\")\n",
    "plt.xlabel(\"Testing Window\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# Appliance and Mains Data\n",
    "appliance_data = {\n",
    "    \"kettle\": {\n",
    "        \"mean\": 700,\n",
    "        \"std\": 1000,\n",
    "        \"houses\": [2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 19, 20],\n",
    "        \"channels\": [8, 9, 9, 8, 7, 9, 9, 7, 6, 9, 5, 9],\n",
    "        \"test_house\": 2,\n",
    "        \"validation_house\": 5,\n",
    "    },\n",
    "    \"fridge\": {\n",
    "        \"mean\": 200,\n",
    "        \"std\": 400,\n",
    "        \"houses\": [2, 5, 9, 12, 15],\n",
    "        \"channels\": [1, 1, 1, 1, 1],\n",
    "        \"test_house\": 15,\n",
    "        \"validation_house\": 12\n",
    "    },\n",
    "    \"dishwasher\": {\n",
    "        \"mean\": 700,\n",
    "        \"std\": 1000,\n",
    "        \"houses\": [5, 7, 9, 13, 16, 18, 20],\n",
    "        \"channels\": [4, 6, 4, 4, 6, 6, 5],\n",
    "        \"test_house\": 9,\n",
    "        \"validation_house\": 18,     \n",
    "    },\n",
    "    \"washingmachine\": {\n",
    "        \"mean\": 400,\n",
    "        \"std\": 700,\n",
    "        \"houses\": [2, 5, 7, 8, 9, 15, 16, 17, 18],\n",
    "        \"channels\": [2, 3, 5, 4, 3, 3, 5, 4, 5],\n",
    "        \"test_house\": 8,\n",
    "        \"validation_house\": 18,\n",
    "    },\n",
    "    \"microwave\": {\n",
    "        \"mean\": 500,\n",
    "        \"std\": 800,\n",
    "        \"houses\": [4, 10, 12, 17, 19],\n",
    "        \"channels\": [8, 8, 3, 7, 4],\n",
    "        \"test_house\": 4,\n",
    "        \"validation_house\": 17,\n",
    "    },\n",
    "}\n",
    "\n",
    "mains_data = {\n",
    "    \"mean\": 522,\n",
    "    \"std\":  814\n",
    "}\n",
    "\n",
    "# Model Parameters\n",
    "input_window_length = 599\n",
    "appliance_name = \"kettle\"\n",
    "algorithm = \"seq2point\"\n",
    "network_type = \"default\"\n",
    "batch_size = 1000\n",
    "crop = 10000\n",
    "test_directory = \"dataset_training/refit/kettle/kettle_test_H2.csv\"\n",
    "saved_model_dir = f\"saved_models/{appliance_name}_{algorithm}_model.h5\"\n",
    "log_file_dir = f\"saved_models/{appliance_name}_{algorithm}_{network_type}.log\"\n",
    "\n",
    "logging.basicConfig(filename=log_file_dir, level=logging.INFO)\n",
    "\n",
    "# Create Model\n",
    "input_layer = tf.keras.layers.Input(shape=(input_window_length,))\n",
    "reshape_layer = tf.keras.layers.Reshape((1, input_window_length, 1))(input_layer)\n",
    "conv_layer_1 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(10, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(reshape_layer)\n",
    "conv_layer_2 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(8, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_1)\n",
    "conv_layer_3 = tf.keras.layers.Convolution2D(filters=40, kernel_size=(6, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_2)\n",
    "conv_layer_4 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_3)\n",
    "conv_layer_5 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_4)\n",
    "flatten_layer = tf.keras.layers.Flatten()(conv_layer_5)\n",
    "label_layer = tf.keras.layers.Dense(1024, activation=\"relu\")(flatten_layer)\n",
    "output_layer = tf.keras.layers.Dense(1, activation=\"linear\")(label_layer)\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Load Model\n",
    "model = tf.keras.models.load_model(saved_model_dir)\n",
    "num_of_weights = model.count_params()\n",
    "print(\"Loaded model with \", str(num_of_weights), \" weights\")\n",
    "\n",
    "# Load Dataset\n",
    "data_frame = pd.read_csv(test_directory, nrows=crop, skiprows=0, header=0)\n",
    "test_input = np.round(np.array(data_frame.iloc[:, 0], float), 6)\n",
    "test_target = np.round(np.array(data_frame.iloc[int(0.5 * (input_window_length + 2) - 1): -int(0.5 * (input_window_length + 2) - 1), 1], float), 6)\n",
    "del data_frame\n",
    "\n",
    "# Test Data Generator\n",
    "window_offset = int(0.5 * (input_window_length + 2) - 1)\n",
    "number_of_windows = 100\n",
    "test_input = test_input.flatten()\n",
    "max_number_of_windows = test_input.size - 2 * window_offset\n",
    "indicies = np.arange(max_number_of_windows, dtype=int)\n",
    "\n",
    "def test_data_generator():\n",
    "    for start_index in range(0, max_number_of_windows, number_of_windows):\n",
    "        splice = indicies[start_index : start_index + number_of_windows]\n",
    "        input_data = np.array([test_input[index : index + 2 * window_offset + 1] for index in splice])\n",
    "        target_data = test_target[splice + window_offset].reshape(-1, 1)\n",
    "        yield input_data, target_data\n",
    "\n",
    "steps_per_test_epoch = np.round(int(len(test_input) / batch_size), decimals=0)\n",
    "\n",
    "# Test Model\n",
    "start_time = time.time()\n",
    "testing_history = model.predict(x=test_data_generator(), steps=steps_per_test_epoch, verbose=2)\n",
    "end_time = time.time()\n",
    "test_time = end_time - start_time\n",
    "\n",
    "evaluation_metrics = model.evaluate(x=test_data_generator(), steps=steps_per_test_epoch)\n",
    "\n",
    "# Log Results\n",
    "inference_log = \"Inference Time: \" + str(test_time)\n",
    "logging.info(inference_log)\n",
    "\n",
    "metric_string = f\"MSE: {evaluation_metrics[0]} MAE: {evaluation_metrics[1]}\"\n",
    "logging.info(metric_string)\n",
    "\n",
    "# Plot Results\n",
    "testing_history = ((testing_history * appliance_data[appliance_name][\"std\"]) + appliance_data[appliance_name][\"mean\"])\n",
    "test_target = ((test_target * appliance_data[appliance_name][\"std\"]) + appliance_data[appliance_name][\"mean\"])\n",
    "test_agg = (test_input.flatten() * mains_data[\"std\"]) + mains_data[\"mean\"]\n",
    "test_agg = test_agg[:testing_history.size]\n",
    "\n",
    "test_target[test_target < 0] = 0\n",
    "testing_history[testing_history < 0] = 0\n",
    "test_input[test_input < 0] = 0\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(test_agg[window_offset: -window_offset], label=\"Aggregate\")\n",
    "plt.plot(test_target[:test_agg.size - (2 * window_offset)], label=\"Ground Truth\")\n",
    "plt.plot(testing_history[:test_agg.size - (2 * window_offset)], label=\"Predicted\")\n",
    "plt.title(appliance_name + \" \" + network_type + \"(\" + algorithm + \")\")\n",
    "plt.ylabel(\"Power Value (Watts)\")\n",
    "plt.xlabel(\"Testing Window\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UKDALE Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    data/ukdale/house_1/channel_6.dat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14184/3313919359.py:124: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  mains_df['time'] = pd.to_datetime(mains_df['time'], unit='s')\n",
      "/tmp/ipykernel_14184/3313919359.py:135: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  app_df['time'] = pd.to_datetime(app_df['time'], unit='s')\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Default Variables\n",
    "DATASET_NAME = \"ukdale\"\n",
    "APPLIANCE_NAME = 'dishwasher'\n",
    "DATA_DIRECTORY = f\"data/{DATASET_NAME}/\"\n",
    "SAVE_PATH = f\"data_train_test/{DATASET_NAME}/{APPLIANCE_NAME}/\"\n",
    "AGG_MEAN = 522\n",
    "AGG_STD = 814\n",
    "AGGREGATE_MEAN = AGG_MEAN\n",
    "AGGREGATE_STD = AGG_STD\n",
    "\n",
    "# Parameters\n",
    "appliance_name = APPLIANCE_NAME\n",
    "data_dir = DATA_DIRECTORY\n",
    "save_path = SAVE_PATH\n",
    "aggregate_mean = AGGREGATE_MEAN\n",
    "aggregate_std = AGGREGATE_STD\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Execution Start\n",
    "start_time = time.time()\n",
    "sample_seconds = 8\n",
    "training_building_percent = 95\n",
    "validation_percent = 13\n",
    "nrows = None\n",
    "debug = False\n",
    "\n",
    "train = pd.DataFrame(columns=['aggregate', appliance_name])\n",
    "\n",
    "params_appliance = {\n",
    "    'kettle': {\n",
    "        'windowlength': 599,\n",
    "        'on_power_threshold': 2000,\n",
    "        'max_on_power': 3998,\n",
    "        'mean': 700,\n",
    "        'std': 1000,\n",
    "        's2s_length': 128,\n",
    "        'houses': [1, 2],\n",
    "        'channels': [10, 8],\n",
    "        'train_build': [1],\n",
    "        'test_build': 2,\n",
    "    },\n",
    "    'microwave': {\n",
    "        'windowlength': 599,\n",
    "        'on_power_threshold': 200,\n",
    "        'max_on_power': 3969,\n",
    "        'mean': 500,\n",
    "        'std': 800,\n",
    "        's2s_length': 128,\n",
    "        'houses': [1, 2],\n",
    "        'channels': [13, 15],\n",
    "        'train_build': [1],\n",
    "        'test_build': 2,\n",
    "    },\n",
    "    'fridge': {\n",
    "        'windowlength': 599,\n",
    "        'on_power_threshold': 50,\n",
    "        'max_on_power': 3323,\n",
    "        'mean': 200,\n",
    "        'std': 400,\n",
    "        's2s_length': 512,\n",
    "        'houses': [1, 2],\n",
    "        'channels': [12, 14],\n",
    "        'train_build': [1],\n",
    "        'test_build': 2,\n",
    "    },\n",
    "    'dishwasher': {\n",
    "        'windowlength': 599,\n",
    "        'on_power_threshold': 10,\n",
    "        'max_on_power': 3964,\n",
    "        'mean': 700,\n",
    "        'std': 1000,\n",
    "        's2s_length': 1536,\n",
    "        'houses': [1, 2],\n",
    "        'channels': [6, 13],\n",
    "        'train_build': [1],\n",
    "        'test_build': 2,\n",
    "    },\n",
    "    'washingmachine': {\n",
    "        'windowlength': 599,\n",
    "        'on_power_threshold': 20,\n",
    "        'max_on_power': 3999,\n",
    "        'mean': 400,\n",
    "        'std': 700,\n",
    "        's2s_length': 2000,\n",
    "        'houses': [1, 2],\n",
    "        'channels': [5, 12],\n",
    "        'train_build': [1],\n",
    "        'test_build': 2,\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_dataframe(directory, building, channel, col_names=['time', 'data'], nrows=None):\n",
    "    df = pd.read_table(directory + 'house_' + str(building) + '/' + 'channel_' +\n",
    "                       str(channel) + '.dat',\n",
    "                       sep=\"\\s+\",\n",
    "                       nrows=nrows,\n",
    "                       usecols=[0, 1],\n",
    "                       names=col_names,\n",
    "                       dtype={'time': str},\n",
    "                       )\n",
    "    return df\n",
    "\n",
    "# Process Data\n",
    "for h in params_appliance[appliance_name]['houses']:\n",
    "    print('    ' + data_dir + 'house_' + str(h) + '/'\n",
    "          + 'channel_' +\n",
    "          str(params_appliance[appliance_name]['channels'][params_appliance[appliance_name]['houses'].index(h)]) +\n",
    "          '.dat')\n",
    "\n",
    "    mains_df = load_dataframe(data_dir, h, 1)\n",
    "    app_df = load_dataframe(data_dir,\n",
    "                            h,\n",
    "                            params_appliance[appliance_name]['channels'][params_appliance[appliance_name]['houses'].index(h)],\n",
    "                            col_names=['time', appliance_name]\n",
    "                            )\n",
    "\n",
    "    mains_df['time'] = pd.to_datetime(mains_df['time'], unit='s')\n",
    "    mains_df.set_index('time', inplace=True)\n",
    "    mains_df.columns = ['aggregate']\n",
    "    mains_df.reset_index(inplace=True)\n",
    "\n",
    "    if debug:\n",
    "        print(\"    mains_df:\")\n",
    "        print(mains_df.head())\n",
    "        plt.plot(mains_df['time'], mains_df['aggregate'])\n",
    "        plt.show()\n",
    "\n",
    "    app_df['time'] = pd.to_datetime(app_df['time'], unit='s')\n",
    "\n",
    "    if debug:\n",
    "        print(\"app_df:\")\n",
    "        print(app_df.head())\n",
    "        plt.plot(app_df['time'], app_df[appliance_name])\n",
    "        plt.show()\n",
    "\n",
    "    # Align timestamps\n",
    "    mains_df.set_index('time', inplace=True)\n",
    "    app_df.set_index('time', inplace=True)\n",
    "\n",
    "    df_align = mains_df.join(app_df, how='outer').resample(str(sample_seconds) + 'S').mean().fillna(method='backfill', limit=1)\n",
    "    df_align = df_align.dropna()\n",
    "    df_align.reset_index(inplace=True)\n",
    "\n",
    "    del mains_df, app_df, df_align['time']\n",
    "\n",
    "    if debug:\n",
    "        print(\"df_align:\")\n",
    "        print(df_align.head())\n",
    "        plt.plot(df_align['aggregate'].values)\n",
    "        plt.plot(df_align[appliance_name].values)\n",
    "        plt.show()\n",
    "\n",
    "    # Normalization\n",
    "    mean = params_appliance[appliance_name]['mean']\n",
    "    std = params_appliance[appliance_name]['std']\n",
    "\n",
    "    df_align['aggregate'] = (df_align['aggregate'] - aggregate_mean) / aggregate_std\n",
    "    df_align[appliance_name] = (df_align[appliance_name] - mean) / std\n",
    "\n",
    "    if h == params_appliance[appliance_name]['test_build']:\n",
    "        df_align.to_csv(save_path + appliance_name + '_test_.csv', mode='a', index=False, header=False)\n",
    "        print(\"    Size of test set is {:.4f} M rows.\".format(len(df_align) / 10 ** 6))\n",
    "        continue\n",
    "\n",
    "    train = pd.concat([train, df_align], ignore_index=True)\n",
    "    del df_align\n",
    "\n",
    "# Crop Training Data\n",
    "if training_building_percent != 0:\n",
    "    train.drop(train.index[-int((len(train)/100)*training_building_percent):], inplace=True)\n",
    "\n",
    "# Validation Set\n",
    "val_len = int((len(train)/100)*validation_percent)\n",
    "val = train.tail(val_len)\n",
    "val.reset_index(drop=True, inplace=True)\n",
    "train.drop(train.index[-val_len:], inplace=True)\n",
    "\n",
    "val.to_csv(save_path + appliance_name + '_validation_' + '.csv', mode='a', index=False, header=False)\n",
    "\n",
    "# Training Set\n",
    "train.to_csv(save_path + appliance_name + '_training_.csv', mode='a', index=False, header=False)\n",
    "\n",
    "print(\"    Size of total training set is {:.4f} M rows.\".format(len(train) / 10 ** 6))\n",
    "print(\"    Size of total validation set is {:.4f} M rows.\".format(len(val) / 10 ** 6))\n",
    "del train, val\n",
    "\n",
    "print(\"\\nPlease find files in: \" + save_path)\n",
    "print(\"Total elapsed time: {:.2f} min.\".format((time.time() - start_time) / 60))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NILM Sequence to Point Model Training with GPU on UKDALE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "import glob\n",
    "\n",
    "# Parameters\n",
    "appliance_name = \"dishwasher\" # kettle, dishwasher, fridge, microwave, washingmachine\n",
    "dataset_name = \"ukdale\"\n",
    "batch_size = 32\n",
    "crop = 1000\n",
    "network_type = \"seq2point\"\n",
    "training_directory = f\"data_train_test/{dataset_name}/{appliance_name}/{appliance_name}_training_.csv\"\n",
    "validation_directory = f\"data_train_test/{dataset_name}/{appliance_name}/{appliance_name}_validation_.csv\"\n",
    "save_model_dir = \"saved_models/\"+ dataset_name + \"/\" + appliance_name + \"_model.h5\"\n",
    "epochs = 10\n",
    "input_window_length = 599\n",
    "validation_frequency = 1\n",
    "patience = 3\n",
    "min_delta = 1e-6\n",
    "verbose = 1\n",
    "loss = \"mse\"\n",
    "metrics = [\"mse\", \"msle\", \"mae\"]\n",
    "learning_rate = 0.001\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "shuffle = True\n",
    "\n",
    "window_size = 2 + input_window_length\n",
    "window_offset = int((0.5 * window_size) - 1)\n",
    "offset = window_offset\n",
    "max_chunk_size = 5 * 10 ** 2\n",
    "ram_threshold = 5 * 10 ** 5\n",
    "skip_rows_train = 10000000\n",
    "validation_steps = 100\n",
    "skip_rows_val = 0\n",
    "\n",
    "# Data Loader Function\n",
    "class TrainSlidingWindowGenerator:\n",
    "    def __init__(self, file_name, chunk_size, shuffle, offset, batch_size=1000, crop=90000, skip_rows=0, ram_threshold=5 * 10 ** 5):\n",
    "        self.file_name = file_name\n",
    "        self.batch_size = batch_size\n",
    "        self.chunk_size = chunk_size\n",
    "        self.shuffle = shuffle\n",
    "        self.offset = offset\n",
    "        self.crop = crop\n",
    "        self.skip_rows = skip_rows\n",
    "        self.ram_threshold = ram_threshold\n",
    "        self.total_size = 0\n",
    "        self.total_num_samples = crop\n",
    "\n",
    "    def load_dataset(self):\n",
    "        print(\"Loading dataset from: \", self.file_name)\n",
    "        try:\n",
    "            data_array = np.array(pd.read_csv(self.file_name, nrows=self.crop, skiprows=self.skip_rows, header=None))\n",
    "            \n",
    "        except:\n",
    "            data_array = np.array(pd.read_csv(self.file_name, header=None, skip_blank_lines=True, delimiter=','))\n",
    "        inputs = data_array[:, 0]\n",
    "        outputs = data_array[:, 1]\n",
    "        maximum_batch_size = inputs.size - 2 * self.offset\n",
    "        self.total_num_samples = maximum_batch_size\n",
    "        if self.batch_size < 0:\n",
    "            self.batch_size = maximum_batch_size\n",
    "\n",
    "        indicies = np.arange(maximum_batch_size)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indicies)\n",
    "\n",
    "        while True:\n",
    "            for start_index in range(0, maximum_batch_size, self.batch_size):\n",
    "                splice = indicies[start_index : start_index + self.batch_size]\n",
    "                input_data = np.array([inputs[index : index + 2 * self.offset + 1] for index in splice])\n",
    "                output_data = outputs[splice + self.offset].reshape(-1, 1)\n",
    "                yield input_data, output_data\n",
    "\n",
    "# Model Creation Function\n",
    "def create_model(input_window_length):\n",
    "    input_layer = tf.keras.layers.Input(shape=(input_window_length,))\n",
    "    reshape_layer = tf.keras.layers.Reshape((1, input_window_length, 1))(input_layer)\n",
    "    conv_layer_1 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(10, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(reshape_layer)\n",
    "    conv_layer_2 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(8, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_1)\n",
    "    conv_layer_3 = tf.keras.layers.Convolution2D(filters=40, kernel_size=(6, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_2)\n",
    "    conv_layer_4 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_3)\n",
    "    conv_layer_5 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_4)\n",
    "    flatten_layer = tf.keras.layers.Flatten()(conv_layer_5)\n",
    "    label_layer = tf.keras.layers.Dense(1024, activation=\"relu\")(flatten_layer)\n",
    "    output_layer = tf.keras.layers.Dense(1, activation=\"linear\")(label_layer)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "# Load Dataset for Training\n",
    "training_chunker = TrainSlidingWindowGenerator(file_name=training_directory, \n",
    "                                               chunk_size=max_chunk_size,\n",
    "                                               batch_size=batch_size,\n",
    "                                               crop=crop,\n",
    "                                               shuffle=shuffle,\n",
    "                                               skip_rows=skip_rows_train,\n",
    "                                               offset=offset,\n",
    "                                               ram_threshold=ram_threshold)\n",
    "\n",
    "# Load Dataset for Validation\n",
    "validation_chunker = TrainSlidingWindowGenerator(file_name=validation_directory, \n",
    "                                                 chunk_size=max_chunk_size,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 crop=crop,\n",
    "                                                 shuffle=False,\n",
    "                                                 skip_rows=skip_rows_val,\n",
    "                                                 offset=offset,\n",
    "                                                 ram_threshold=ram_threshold)\n",
    "\n",
    "# Create and Compile the Model\n",
    "model = create_model(input_window_length)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2), \n",
    "              loss=loss, \n",
    "              metrics=metrics)\n",
    "\n",
    "# Set Early Stopping Callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", min_delta=min_delta, patience=patience, verbose=verbose, mode=\"auto\")\n",
    "callbacks = [early_stopping]\n",
    "\n",
    "# Calculate Steps per Epoch\n",
    "steps_per_training_epoch = np.round(int(training_chunker.total_num_samples / batch_size), decimals=0) if training_chunker.total_num_samples is not None else 1\n",
    "\n",
    "# Train the Model\n",
    "training_history = model.fit(training_chunker.load_dataset(),\n",
    "                             validation_data=validation_chunker.load_dataset(),\n",
    "                             steps_per_epoch=steps_per_training_epoch,\n",
    "                             epochs=epochs,\n",
    "                             verbose=verbose,\n",
    "                             callbacks=callbacks,\n",
    "                             validation_steps=validation_steps,\n",
    "                             validation_freq=validation_frequency)\n",
    "\n",
    "# Save the Model\n",
    "if not os.path.exists(save_model_dir):\n",
    "    open((save_model_dir), 'a').close()\n",
    "model.save(save_model_dir)\n",
    "\n",
    "# Plot Training Results\n",
    "plt.plot(training_history.history[\"loss\"], label=\"MSE (Training Loss)\")\n",
    "if \"val_loss\" in training_history.history:\n",
    "    plt.plot(training_history.history[\"val_loss\"], label=\"MSE (Validation Loss)\")\n",
    "plt.title('Training History')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "del model\n",
    "# del test_input, test_target, input_data, testing_history\n",
    "gc.collect()\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NILM Sequence to Point Model Inference with GPU on UKDALE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "import numpy as np \n",
    "import keras\n",
    "import pandas as pd\n",
    "import tensorflow as tf \n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "appliance_data = {\n",
    "    \"kettle\": {\n",
    "        \"mean\": 700,\n",
    "        \"std\": 1000,\n",
    "        \"houses\": [2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 19, 20],\n",
    "        \"channels\": [8, 9, 9, 8, 7, 9, 9, 7, 6, 9, 5, 9],\n",
    "        \"test_house\": 2,\n",
    "        \"validation_house\": 5,\n",
    "    },\n",
    "    \"fridge\": {\n",
    "        \"mean\": 200,\n",
    "        \"std\": 400,\n",
    "        \"houses\": [2, 5, 9, 12, 15],\n",
    "        \"channels\": [1, 1, 1, 1, 1],\n",
    "        \"test_house\": 15,\n",
    "        \"validation_house\": 12\n",
    "    },\n",
    "    \"dishwasher\": {\n",
    "        \"mean\": 700,\n",
    "        \"std\": 1000,\n",
    "        \"houses\": [5, 7, 9, 13, 16, 18, 20],\n",
    "        \"channels\": [4, 6, 4, 4, 6, 6, 5],\n",
    "        \"test_house\": 9,\n",
    "        \"validation_house\": 18,     \n",
    "    },\n",
    "    \"washingmachine\": {\n",
    "        \"mean\": 400,\n",
    "        \"std\": 700,\n",
    "        \"houses\": [2, 5, 7, 8, 9, 15, 16, 17, 18],\n",
    "        \"channels\": [2, 3, 5, 4, 3, 3, 5, 4, 5],\n",
    "        \"test_house\": 8,\n",
    "        \"validation_house\": 18,\n",
    "    },\n",
    "    \"microwave\": {\n",
    "        \"mean\": 500,\n",
    "        \"std\": 800,\n",
    "        \"houses\": [4, 10, 12, 17, 19],\n",
    "        \"channels\": [8, 8, 3, 7, 4],\n",
    "        \"test_house\": 4,\n",
    "        \"validation_house\": 17,\n",
    "    },\n",
    "}\n",
    "\n",
    "mains_data = {\n",
    "    \"mean\": 522,\n",
    "    \"std\":  814        \n",
    "    }\n",
    "\n",
    "class TrainSlidingWindowGenerator():\n",
    "\n",
    "    def __init__(self, \n",
    "                file_name, \n",
    "                chunk_size, \n",
    "                shuffle, \n",
    "                offset, \n",
    "                batch_size=1000, \n",
    "                crop=100000, \n",
    "                skip_rows=0, \n",
    "                ram_threshold=5 * 10 ** 5):\n",
    "        self.__file_name = file_name\n",
    "        self.__batch_size = batch_size\n",
    "        self.__chunk_size = 10 ** 8\n",
    "        self.__shuffle = shuffle\n",
    "        self.__offset = offset\n",
    "        self.__crop = crop\n",
    "        self.__skip_rows = skip_rows\n",
    "        self.__ram_threshold = ram_threshold\n",
    "        self.total_size = 0\n",
    "        self.__total_num_samples = crop\n",
    "\n",
    "    @property\n",
    "    def total_num_samples(self):\n",
    "        return self.__total_num_samples\n",
    "    \n",
    "    @total_num_samples.setter\n",
    "    def total_num_samples(self, value):\n",
    "        self.__total_num_samples = value\n",
    "\n",
    "    def check_if_chunking(self):\n",
    "\n",
    "        # Loads the file and counts the number of rows it contains.\n",
    "        print(\"Importing training file...\")\n",
    "        chunks = pd.read_csv(self.__file_name, \n",
    "                            header=0, \n",
    "                            nrows=self.__crop, \n",
    "                            skiprows=self.__skip_rows)\n",
    "        print(\"Counting number of rows...\")\n",
    "        self.total_size = len(chunks)\n",
    "        del chunks\n",
    "        print(\"Done.\")\n",
    "\n",
    "        print(\"The dataset contains \", self.total_size, \" rows\")\n",
    "\n",
    "        # Display a warning if there are too many rows to fit in the designated amount RAM.\n",
    "        if (self.total_size > self.__ram_threshold):\n",
    "            print(\"There is too much data to load into memory, so it will be loaded in chunks. Please note that this may result in decreased training times.\")\n",
    "    \n",
    "\n",
    "    def load_dataset(self):\n",
    "\n",
    "        if self.total_size == 0:\n",
    "            self.check_if_chunking()\n",
    "\n",
    "        # If the data can be loaded in one go, don't skip any rows.\n",
    "        if (self.total_size <= self.__ram_threshold):\n",
    "\n",
    "            # Returns an array of the content from the CSV file.\n",
    "            data_array = np.array(pd.read_csv(self.__file_name, nrows=self.__crop, skiprows=self.__skip_rows, header=0))\n",
    "            inputs = data_array[:, 0]\n",
    "            outputs = data_array[:, 1]\n",
    "\n",
    "            maximum_batch_size = inputs.size - 2 * self.__offset\n",
    "            self.total_num_samples = maximum_batch_size\n",
    "            if self.__batch_size < 0:\n",
    "                self.__batch_size = maximum_batch_size\n",
    "\n",
    "            indicies = np.arange(maximum_batch_size)\n",
    "            if self.__shuffle:\n",
    "                np.random.shuffle(indicies)\n",
    "\n",
    "            while True:\n",
    "                for start_index in range(0, maximum_batch_size, self.__batch_size):\n",
    "                    splice = indicies[start_index : start_index + self.__batch_size]\n",
    "                    input_data = np.array([inputs[index : index + 2 * self.__offset + 1] for index in splice])\n",
    "                    output_data = outputs[splice + self.__offset].reshape(-1, 1)\n",
    "\n",
    "                    yield input_data, output_data\n",
    "                    \n",
    "        # Skip rows where needed to allow data to be loaded properly when there is not enough memory.\n",
    "        if (self.total_size >= self.__ram_threshold):\n",
    "            number_of_chunks = np.arange(self.total_size / self.__chunk_size)\n",
    "            if self.__shuffle:\n",
    "                np.random.shuffle(number_of_chunks)\n",
    "\n",
    "            # Yield the data in sections.\n",
    "            for index in number_of_chunks:\n",
    "                data_array = np.array(pd.read_csv(self.__file_name, skiprows=int(index) * self.__chunk_size, header=0, nrows=self.__crop))                   \n",
    "                inputs = data_array[:, 0]\n",
    "                outputs = data_array[:, 1]\n",
    "\n",
    "                maximum_batch_size = inputs.size - 2 * self.__offset\n",
    "                self.total_num_samples = maximum_batch_size\n",
    "                if self.__batch_size < 0:\n",
    "                    self.__batch_size = maximum_batch_size\n",
    "\n",
    "                indicies = np.arange(maximum_batch_size)\n",
    "                if self.__shuffle:\n",
    "                    np.random.shuffle(indicies)\n",
    "\n",
    "            while True:\n",
    "                for start_index in range(0, maximum_batch_size, self.__batch_size):\n",
    "                    splice = indicies[start_index : start_index + self.__batch_size]\n",
    "                    input_data = np.array([inputs[index : index + 2 * self.__offset + 1] for index in splice])\n",
    "                    output_data = outputs[splice + self.__offset].reshape(-1, 1)\n",
    "\n",
    "                    yield input_data, output_data\n",
    "                    \n",
    "class TestSlidingWindowGenerator(object):\n",
    "\n",
    "    def __init__(self, number_of_windows, inputs, targets, offset):\n",
    "        self.__number_of_windows = number_of_windows\n",
    "        self.__offset = offset\n",
    "        self.__inputs = inputs\n",
    "        self.__targets = targets\n",
    "        self.total_size = len(inputs)\n",
    "\n",
    "    def load_dataset(self):\n",
    "\n",
    "        self.__inputs = self.__inputs.flatten()\n",
    "        max_number_of_windows = self.__inputs.size - 2 * self.__offset\n",
    "\n",
    "        if self.__number_of_windows < 0:\n",
    "            self.__number_of_windows = max_number_of_windows\n",
    "\n",
    "        indicies = np.arange(max_number_of_windows, dtype=int)\n",
    "        for start_index in range(0, max_number_of_windows, self.__number_of_windows):\n",
    "            splice = indicies[start_index : start_index + self.__number_of_windows]\n",
    "            input_data = np.array([self.__inputs[index : index + 2 * self.__offset + 1] for index in splice])\n",
    "            target_data = self.__targets[splice + self.__offset].reshape(-1, 1)\n",
    "            yield input_data, target_data\n",
    "\n",
    "def create_model(input_window_length):\n",
    "\n",
    "    input_layer = tf.keras.layers.Input(shape=(input_window_length,))\n",
    "    reshape_layer = tf.keras.layers.Reshape((1, input_window_length, 1))(input_layer)\n",
    "    conv_layer_1 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(10, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(reshape_layer)\n",
    "    conv_layer_2 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(8, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_1)\n",
    "    conv_layer_3 = tf.keras.layers.Convolution2D(filters=40, kernel_size=(6, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_2)\n",
    "    conv_layer_4 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_3)\n",
    "    conv_layer_5 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_4)\n",
    "    flatten_layer = tf.keras.layers.Flatten()(conv_layer_5)\n",
    "    label_layer = tf.keras.layers.Dense(1024, activation=\"relu\")(flatten_layer)\n",
    "    output_layer = tf.keras.layers.Dense(1, activation=\"linear\")(label_layer)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "def load_model(model, network_type, algorithm, appliance, saved_model_dir):\n",
    "\n",
    "    #model_name = \"saved_models/\" + appliance + \"_\" + algorithm + \"_\" + network_type + \"_model.h5\"\n",
    "    model_name = saved_model_dir\n",
    "    print(\"PATH NAME: \", model_name)\n",
    "\n",
    "    model = tf.keras.models.load_model(model_name)\n",
    "    num_of_weights = model.count_params()\n",
    "    print(\"Loaded model with \", str(num_of_weights), \" weights\")\n",
    "    return model\n",
    "\n",
    "class Tester():\n",
    "\n",
    "    def __init__(self, appliance, algorithm, crop, batch_size, network_type,\n",
    "                 test_directory, saved_model_dir, log_file_dir,\n",
    "                 input_window_length):\n",
    "        self.__appliance = appliance\n",
    "        self.__algorithm = algorithm\n",
    "        self.__network_type = network_type\n",
    "\n",
    "        self.__crop = crop\n",
    "        self.__batch_size = batch_size\n",
    "        self._input_window_length = input_window_length\n",
    "        self.__window_size = self._input_window_length + 2\n",
    "        self.__window_offset = int(0.5 * self.__window_size - 1)\n",
    "        self.__number_of_windows = 100\n",
    "\n",
    "        self.__test_directory = test_directory\n",
    "        self.__saved_model_dir = saved_model_dir\n",
    "\n",
    "        self.__log_file = log_file_dir\n",
    "        logging.basicConfig(filename=self.__log_file,level=logging.INFO)\n",
    "\n",
    "    def test_model(self):\n",
    "\n",
    "        test_input, test_target = self.load_dataset(self.__test_directory)\n",
    "        model = create_model(self._input_window_length)\n",
    "        model = load_model(model, self.__network_type, self.__algorithm, \n",
    "                           self.__appliance, self.__saved_model_dir)\n",
    "\n",
    "        test_generator = TestSlidingWindowGenerator(number_of_windows=self.__number_of_windows, inputs=test_input, targets=test_target, offset=self.__window_offset)\n",
    "\n",
    "        # Calculate the optimum steps per epoch.\n",
    "        steps_per_test_epoch = np.round(int(test_generator.total_size / self.__batch_size), decimals=0)\n",
    "\n",
    "        # Test the model.\n",
    "        start_time = time.time()\n",
    "        testing_history = model.predict(x=test_generator.load_dataset(), steps=steps_per_test_epoch, verbose=2)\n",
    "\n",
    "        end_time = time.time()\n",
    "        test_time = end_time - start_time\n",
    "\n",
    "        evaluation_metrics = model.evaluate(x=test_generator.load_dataset(), steps=steps_per_test_epoch)\n",
    "\n",
    "        self.log_results(model, test_time, evaluation_metrics)\n",
    "        self.plot_results(testing_history, test_input, test_target)\n",
    "\n",
    "\n",
    "    def load_dataset(self, directory):\n",
    "\n",
    "        data_frame = pd.read_csv(directory, nrows=self.__crop, skiprows=0, header=0)\n",
    "        test_input = np.round(np.array(data_frame.iloc[:, 0], float), 6)\n",
    "        test_target = np.round(np.array(data_frame.iloc[self.__window_offset: -self.__window_offset, 1], float), 6)\n",
    "        \n",
    "        del data_frame\n",
    "        return test_input, test_target\n",
    "\n",
    "    def log_results(self, model, test_time, evaluation_metrics):\n",
    "\n",
    "        inference_log = \"Inference Time: \" + str(test_time)\n",
    "        logging.info(inference_log)\n",
    "\n",
    "        metric_string = \"MSE: \", str(evaluation_metrics[0]), \" MAE: \", str(evaluation_metrics[3])\n",
    "        logging.info(metric_string)\n",
    "\n",
    "        self.count_pruned_weights(model)  \n",
    "\n",
    "    \n",
    "    def count_pruned_weights(self, model):\n",
    "        num_total_zeros = 0\n",
    "        num_dense_zeros = 0\n",
    "        num_dense_weights = 0\n",
    "        num_conv_zeros = 0\n",
    "        num_conv_weights = 0\n",
    "\n",
    "        for layer in model.layers:\n",
    "            weights = layer.get_weights()\n",
    "            if weights:  # Check if the layer has weights\n",
    "                for w in weights:\n",
    "                    if np.asarray(w).size != 0:  # Ensure it's a non-empty array\n",
    "                        w_array = np.asarray(w).flatten()\n",
    "\n",
    "                        if \"conv\" in layer.name:\n",
    "                            num_conv_weights += np.size(w_array)\n",
    "                            num_conv_zeros += np.count_nonzero(w_array == 0)\n",
    "\n",
    "                            num_total_zeros += np.size(w_array)\n",
    "                        else:\n",
    "                            num_dense_weights += np.size(w_array)\n",
    "                            num_dense_zeros += np.count_nonzero(w_array == 0)\n",
    "\n",
    "        conv_zeros_string = \"CONV. ZEROS: \" + str(num_conv_zeros)\n",
    "        conv_weights_string = \"CONV. WEIGHTS: \" + str(num_conv_weights)\n",
    "        conv_sparsity_ratio = \"CONV. RATIO: \" + str(num_conv_zeros / num_conv_weights if num_conv_weights > 0 else 0)\n",
    "\n",
    "        dense_weights_string = \"DENSE WEIGHTS: \" + str(num_dense_weights)\n",
    "        dense_zeros_string = \"DENSE ZEROS: \" + str(num_dense_zeros)\n",
    "        dense_sparsity_ratio = \"DENSE RATIO: \" + str(num_dense_zeros / num_dense_weights if num_dense_weights > 0 else 0)\n",
    "\n",
    "        total_zeros_string = \"TOTAL ZEROS: \" + str(num_total_zeros)\n",
    "        total_weights_string = \"TOTAL WEIGHTS: \" + str(model.count_params())\n",
    "        total_sparsity_ratio = \"TOTAL RATIO: \" + str(num_total_zeros / model.count_params())\n",
    "\n",
    "        print(\"LOGGING PATH: \", self.__log_file)\n",
    "\n",
    "        logging.info(conv_zeros_string)\n",
    "        logging.info(conv_weights_string)\n",
    "        logging.info(conv_sparsity_ratio)\n",
    "        logging.info(\"\")\n",
    "        logging.info(dense_zeros_string)\n",
    "        logging.info(dense_weights_string)\n",
    "        logging.info(dense_sparsity_ratio)\n",
    "        logging.info(\"\")\n",
    "        logging.info(total_zeros_string)\n",
    "        logging.info(total_weights_string)\n",
    "        logging.info(total_sparsity_ratio)\n",
    "\n",
    "\n",
    "    def plot_results(self, testing_history, test_input, test_target):\n",
    "\n",
    "        testing_history = ((testing_history * appliance_data[self.__appliance][\"std\"]) + appliance_data[self.__appliance][\"mean\"])\n",
    "        test_target = ((test_target * appliance_data[self.__appliance][\"std\"]) + appliance_data[self.__appliance][\"mean\"])\n",
    "        test_agg = (test_input.flatten() * mains_data[\"std\"]) + mains_data[\"mean\"]\n",
    "        test_agg = test_agg[:testing_history.size]\n",
    "\n",
    "        # Can't have negative energy readings - set any results below 0 to 0.\n",
    "        test_target[test_target < 0] = 0\n",
    "        testing_history[testing_history < 0] = 0\n",
    "        test_input[test_input < 0] = 0\n",
    "\n",
    "        plt.figure(1)\n",
    "        plt.plot(test_agg[self.__window_offset: -self.__window_offset], label=\"Aggregate\")\n",
    "        plt.plot(test_target[:test_agg.size - (2 * self.__window_offset)], label=\"Ground Truth\")\n",
    "        plt.plot(testing_history[:test_agg.size - (2 * self.__window_offset)], label=\"Predicted\")\n",
    "        plt.title(self.__appliance + \" \" + self.__network_type + \"(\" + self.__algorithm + \")\")\n",
    "        plt.ylabel(\"Power Value (Watts)\")\n",
    "        plt.xlabel(\"Testing Window\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "def remove_space(string):\n",
    "    return string.replace(\" \",\"\")\n",
    "\n",
    "appliance_name = \"kettle\"\n",
    "dataset_name = \"ukdale\"\n",
    "batch_size = 1000\n",
    "crop = 10000\n",
    "algorithm = \"seq2point\"\n",
    "network_type = \"\"\n",
    "input_window_length = 599\n",
    "test_directory = f\"data_train_test/{dataset_name}/{appliance_name}/{appliance_name}_test_.csv\"\n",
    "saved_model_dir = f\"saved_models/{dataset_name}/{appliance_name}_model.h5\"\n",
    "log_file_dir = f\"saved_models/{dataset_name}/{appliance_name}_{algorithm}_{network_type}.log\"\n",
    "\n",
    "tester = Tester(\n",
    "    appliance=appliance_name,\n",
    "    algorithm=algorithm,\n",
    "    crop=crop,\n",
    "    batch_size=batch_size,\n",
    "    network_type=network_type,\n",
    "    test_directory=test_directory,\n",
    "    saved_model_dir=saved_model_dir,\n",
    "    log_file_dir=log_file_dir,\n",
    "    input_window_length=input_window_length\n",
    ")\n",
    "tester.test_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nilm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
